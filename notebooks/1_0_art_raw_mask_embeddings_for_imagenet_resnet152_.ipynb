{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "387RjGInbccs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unHO__UZWGQL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip install and authenticate"
      ],
      "metadata": {
        "id": "NBlOx9mEHOp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies.\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt -qq update && apt -qq install gcsfuse\n",
        "\n",
        "!pip install matplotlib nibabel -q\n",
        "\n",
        "# Authenticate.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YECkGx2bHVAo",
        "outputId": "184af417-5082-4c4f-b5a8-9912eec07597"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2659  100  2659    0     0  11027      0 --:--:-- --:--:-- --:--:-- 11079\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "17 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "gcsfuse is already the newest version (1.2.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount a Cloud Storage bucket or location, without the gs:// prefix.\n",
        "mount_path = \"brats-image-files-eu\"  # or a location like \"my-bucket/path/to/mount\"\n",
        "local_path = f\"/mnt/gs/{mount_path}\"\n",
        "\n",
        "!mkdir -p {local_path}\n",
        "!gcsfuse --implicit-dirs {mount_path} {local_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7eOtSNAqDom",
        "outputId": "9c6b2794-707a-42c3-a52e-68a349442d0b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"time\":\"30/11/2023 07:23:56.537653\",\"severity\":\"INFO\",\"msg\":\"Start gcsfuse/1.2.1 (Go version go1.21.3) for app \\\"\\\" using mount point: /mnt/gs/brats-image-files-eu\\n\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount a Cloud Storage bucket or location, without the gs:// prefix.\n",
        "mount_path = \"picture_embeddings/imagenet_resnets\"  # or a location like \"my-bucket/path/to/mount\"\n",
        "local_path = f\"/mnt/gs/{mount_path}\"\n",
        "\n",
        "!mkdir -p {local_path}\n",
        "!gcsfuse --implicit-dirs {mount_path} {local_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAzvpyafhlQS",
        "outputId": "a8efbe4b-54ef-413d-b0c4-e55f5c44255e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"time\":\"30/11/2023 07:23:58.775562\",\"severity\":\"INFO\",\"msg\":\"Start gcsfuse/1.2.1 (Go version go1.21.3) for app \\\"\\\" using mount point: /mnt/gs/picture_embeddings/imagenet_resnets\\n\"}\n",
            "daemonize.Run: readFromProcess: sub-process: mountWithArgs: mountWithStorageHandle: fs.NewServer: create file system: SetUpBucket: Error in iterating through objects: storage: bucket doesn't exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JHD68hICqMXZ",
        "outputId": "0ce5a204-755a-4889-8413-7c0fc7a9798c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/gs/picture_embeddings/imagenet_resnets'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_nii(file_name):\n",
        "    '''\n",
        "    Reads a NIfTI file and returns the data as a numpy array\n",
        "    '''\n",
        "\n",
        "    # reads the NIfTI file\n",
        "    nii_file = nib.load(file_name)\n",
        "\n",
        "    # Access the data\n",
        "    data = nii_file.get_fdata()\n",
        "    return data\n",
        "\n",
        "\n",
        "def normalize_nii_for_pil(array):\n",
        "  # Normalize the array to 0-255\n",
        "  array_normalized = (array - array.min()) / (array.max() - array.min()) * 255\n",
        "\n",
        "  # Convert to uint8\n",
        "  array_uint8 = array_normalized.astype(np.uint8)\n",
        "\n",
        "  return array_uint8"
      ],
      "metadata": {
        "id": "Nqhp6hWMiinf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_slice_with_max_mask_area(mask_data):\n",
        "        '''\n",
        "        Function to display the slice with the largest affected\n",
        "        area of the original MRI scan together with slice of segmentation mask\n",
        "\n",
        "        Args:\n",
        "            mask_data (numpy array): segmentation mask (already loaded from NIfTI file)\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "\n",
        "        # Initialize variables to track the largest slice\n",
        "        max_non_black_count = 0\n",
        "        max_slice_index = 0\n",
        "\n",
        "        # Iterate through each slice in the mask\n",
        "        for i in range(mask_data.shape[2]):\n",
        "            # Count non-black (non-zero) pixels in the slice\n",
        "            non_black_count = np.count_nonzero(mask_data[:, :, i])\n",
        "\n",
        "            # Update max count and slice index if current slice has more non-black pixels\n",
        "            if non_black_count > max_non_black_count:\n",
        "                max_non_black_count = non_black_count\n",
        "                max_slice_index = i\n",
        "\n",
        "        ## Get bounding-box from mask\n",
        "        mask_array = mask_data[:,:,max_slice_index]\n",
        "        # Identifying the indices of non-zero elements\n",
        "        non_zero_indices = np.argwhere(mask_array != 0)\n",
        "\n",
        "        return max_slice_index"
      ],
      "metadata": {
        "id": "6Js6GXMy8j1R"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/mnt/gs/brats-image-files-eu/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKF2xgpX5QCJ",
        "outputId": "3b4299aa-78fe-4756-d3e2-0d71db31ace8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1251"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/mnt/gs/brats-image-files-eu/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiCqGYSO5ZRi",
        "outputId": "8996a7be-f0f4-448d-8f49-043345e694ce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/mnt/gs/brats-image-files-eu/BraTS-MEN-Train'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTBdJVFp5gmr",
        "outputId": "9f5f2f72-3c9e-4bcc-8dc4-974b0b72dfbb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "parent_folder = Path('/mnt/gs/brats-image-files-eu')\n",
        "\n",
        "dataset_folders = os.listdir(parent_folder)"
      ],
      "metadata": {
        "id": "RY5JGSDOGcBB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modalities = ['t1', 't2', 'flair', 't1c']\n",
        "patient_folders = os.listdir(parent_folder / dataset_folders[0])\n",
        "filename = Path(patient_folders[0]) / f'{patient_folders[0]}_raw_cutout_{modalities[0]}.png'\n",
        "parent_folder / dataset_folders[0] / patient_folders[0] / 'png' / filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxfa-Q8OHlcO",
        "outputId": "c7f7e50f-b149-48ba-8fd7-4f88c384831a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/mnt/gs/brats-image-files-eu/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/BraTS-GLI-00000-000/png/BraTS-GLI-00000-000/BraTS-GLI-00000-000_raw_cutout_t1.png')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate embeddings"
      ],
      "metadata": {
        "id": "FDLzZbkCHUQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/christiansafka/img2vec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "class Img2Vec():\n",
        "    RESNET_OUTPUT_SIZES = {\n",
        "        'resnet18': 512,\n",
        "        'resnet34': 512,\n",
        "        'resnet50': 2048,\n",
        "        'resnet101': 2048,\n",
        "        'resnet152': 2048,\n",
        "    }\n",
        "\n",
        "    def __init__(self, cuda=False, model='resnet-34', layer='default',\n",
        "                 layer_output_size=512):\n",
        "        \"\"\" Img2Vec\n",
        "        :param cuda: If set to True, will run forward pass on GPU\n",
        "        :param model: String name of requested model\n",
        "        :param layer: String or Int depending on model.  See more docs: https://github.com/christiansafka/img2vec.git\n",
        "        :param layer_output_size: Int depicting the output size of the requested layer\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "        self.layer_output_size = layer_output_size\n",
        "        self.model_name = model\n",
        "\n",
        "        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        self.scaler = transforms.Resize((224, 224))\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def get_vec(self, img, tensor=False):\n",
        "        \"\"\" Get vector embedding from PIL image\n",
        "        :param img: PIL Image or list of PIL Images\n",
        "        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n",
        "        :returns: Numpy ndarray\n",
        "        \"\"\"\n",
        "        if type(img) == list:\n",
        "            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n",
        "            images = torch.stack(a).to(self.device)\n",
        "            if self.model_name == 'alexnet':\n",
        "                my_embedding = torch.zeros(len(img), self.layer_output_size)\n",
        "            else:\n",
        "                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n",
        "\n",
        "            def copy_data(m, i, o):\n",
        "                my_embedding.copy_(o.data)\n",
        "\n",
        "            h = self.extraction_layer.register_forward_hook(copy_data)\n",
        "            h_x = self.model(images)\n",
        "            h.remove()\n",
        "\n",
        "            if tensor:\n",
        "                return my_embedding\n",
        "            else:\n",
        "                if self.model_name == 'alexnet':\n",
        "                    return my_embedding.numpy()[:, :]\n",
        "                else:\n",
        "                    return my_embedding.numpy()[:, :, 0, 0]\n",
        "        else:\n",
        "            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(self.device)\n",
        "\n",
        "            if self.model_name == 'alexnet':\n",
        "                my_embedding = torch.zeros(1, self.layer_output_size)\n",
        "            else:\n",
        "                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n",
        "\n",
        "            def copy_data(m, i, o):\n",
        "                my_embedding.copy_(o.data)\n",
        "\n",
        "            h = self.extraction_layer.register_forward_hook(copy_data)\n",
        "            h_x = self.model(image)\n",
        "            h.remove()\n",
        "\n",
        "            if tensor:\n",
        "                return my_embedding\n",
        "            else:\n",
        "                if self.model_name == 'alexnet':\n",
        "                    return my_embedding.numpy()[0, :]\n",
        "                else:\n",
        "                    return my_embedding.numpy()[0, :, 0, 0]\n",
        "\n",
        "    def _get_model_and_layer(self, model_name, layer):\n",
        "        \"\"\" Internal method for getting layer from model\n",
        "        :param model_name: model name such as 'resnet-18'\n",
        "        :param layer: layer as a string for resnet-18 or int for alexnet\n",
        "        :returns: pytorch model, selected layer\n",
        "        \"\"\"\n",
        "\n",
        "        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n",
        "            model = getattr(models, model_name)(pretrained=True)\n",
        "            if layer == 'default':\n",
        "                layer = model._modules.get('avgpool')\n",
        "                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n",
        "            else:\n",
        "                layer = model._modules.get(layer)\n",
        "            return model, layer\n",
        "        elif model_name == 'resnet-34':\n",
        "            model = models.resnet34(pretrained=True)\n",
        "            if layer == 'default':\n",
        "                layer = model._modules.get('avgpool')\n",
        "                self.layer_output_size = 512\n",
        "            else:\n",
        "                layer = model._modules.get(layer)\n",
        "\n",
        "            return model, layer\n",
        "\n",
        "        elif model_name == 'alexnet':\n",
        "            model = models.alexnet(pretrained=True)\n",
        "            if layer == 'default':\n",
        "                layer = model.classifier[-2]\n",
        "                self.layer_output_size = 4096\n",
        "            else:\n",
        "                layer = model.classifier[-layer]\n",
        "\n",
        "            return model, layer\n",
        "\n",
        "        else:\n",
        "            raise KeyError('Model %s was not found' % model_name)"
      ],
      "metadata": {
        "id": "LN-6FSKSc6xf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "img2vec152 = Img2Vec(model='resnet152')"
      ],
      "metadata": {
        "id": "1IiUbQtWjuBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea1748a-f8c6-463e-fbce-3c4ed5ced6ef"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long generation process"
      ],
      "metadata": {
        "id": "fVOdqEJAsdLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "def get_cutout(data, mask, slice_number):\n",
        "  return data[:, :, slice_number] * mask[:, :, slice_number]\n",
        "\n",
        "def png_cutout_save_path(patient_name, modality):\n",
        "  return f'{patient_name}_{modality}_raw_cutout.png'\n",
        "\n",
        "image_embeddings = {}\n",
        "\n",
        "for dataset_folder in os.listdir(parent_folder):\n",
        "  for patient_folder in os.listdir(parent_folder / dataset_folder):\n",
        "    for patient_files in os.listdir(parent_folder / dataset_folder / patient_folder):\n",
        "        full_path_patient_folder = parent_folder / dataset_folder / patient_folder\n",
        "        seg_filename = f'{str(patient_folder)}-seg.nii.gz'\n",
        "        t1_filename = f'{str(patient_folder)}-t1n.nii.gz'\n",
        "        t1_gad_filename = f'{str(patient_folder)}-t1c.nii.gz'\n",
        "        t2_filename = f'{str(patient_folder)}-t2w.nii.gz'\n",
        "        flair_filename = f'{str(patient_folder)}-t2f.nii.gz'\n",
        "\n",
        "        seg_data =    (read_nii(full_path_patient_folder / seg_filename) >=1).astype(np.uint8)\n",
        "        seg_data_w_levels =    read_nii(full_path_patient_folder / seg_filename).astype(np.uint8)\n",
        "\n",
        "\n",
        "        t1_data =     normalize_nii_for_pil(read_nii(full_path_patient_folder / t1_filename))\n",
        "        t2_data =     normalize_nii_for_pil(read_nii(full_path_patient_folder / t2_filename))\n",
        "        t1_gad_data = normalize_nii_for_pil(read_nii(full_path_patient_folder / t1_gad_filename))\n",
        "        flair_data =  normalize_nii_for_pil(read_nii(full_path_patient_folder / flair_filename))\n",
        "\n",
        "        slice_index = find_slice_with_max_mask_area(seg_data)\n",
        "\n",
        "        flair_cutout = Image.fromarray(get_cutout(flair_data, seg_data, slice_index)).convert('RGB')\n",
        "        t1_cutout = Image.fromarray(get_cutout(t1_data, seg_data, slice_index)).convert('RGB')\n",
        "        t2_cutout = Image.fromarray(get_cutout(t2_data, seg_data, slice_index)).convert('RGB')\n",
        "        t1_gad_cutout = Image.fromarray(get_cutout(t1_gad_data, seg_data, slice_index)).convert('RGB')\n",
        "\n",
        "       #generate embeddings and store in dictionary here\n",
        "        image_embeddings[full_path_patient_folder / 'png'/png_cutout_save_path(patient_folder, 'flair')] = img2vec152.get_vec(flair_cutout)\n",
        "        image_embeddings[full_path_patient_folder / 'png'/png_cutout_save_path(patient_folder, 't1')] = img2vec152.get_vec(t1_cutout)\n",
        "        image_embeddings[full_path_patient_folder / 'png'/png_cutout_save_path(patient_folder, 't1c')] = img2vec152.get_vec(t1_gad_cutout)\n",
        "        image_embeddings[full_path_patient_folder / 'png'/png_cutout_save_path(patient_folder, 't2')] = img2vec152.get_vec(t2_cutout)\n",
        "\n",
        "        # delete breaks to make notebook run\n",
        "        break\n",
        "    break\n",
        "  break"
      ],
      "metadata": {
        "id": "2VDse6uDZ9od"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.open(list(image_embeddings.keys())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "8eqJ4s0jktYJ",
        "outputId": "7640ba42-26dc-4841-9285-950ae0476482"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=240x240>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPAAAADwCAIAAACxN37FAAASlElEQVR4nO3dy28kV/UH8FvvR1d1V7887VfPjDNJhiCygEjDhgX/AzvWbJBCRBSYREEj4gQyCQERRiKgIMECxAI2LFghVoAAESRImEGaIMeJx3bb/e6q7npX/RZfYRKUHwJ+/Gh75vtZTSLbapdO3z73nHOvhSAiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiI6VaRlv4D/pO3t7TAMkyTJ87wsy7Isv/GNbyz7RdF/1d0T0M8991xRFLquy7IcRVGSJJZlqaoqSZIsy4vFYj6fJ0lSFIUsy2VZhmE4HA77/f758+c7nc5isciyLE3TxWKhKMr3v//9Zf9C9O+4SwL62rVrQghVVWu1miRJURTleR6GYRRFqqoKIbBm67puWdZ0Ou33+47jaJoWhmGtVsuybDAYWJaVpmmSJJVKJUmS2WwmhPjRj3605N+N/hXqsl/Af0YQBLVarVqtNhoNSZLm87mqqkmS+L6f53me53EcB0FQlqVlWfV63TTNPM+LojBNU1XVxWIhSVIcx3meN5vNRqMxGAyEEHEcL/s3o3/N2V6hr169qmmaJEl5nquq6rpup9NRVTWKItM0ZVmO4xi5RJIkQRBompYkyWQy8TyvXq+naZq/S5ZlYRjKsmxZFlb3MAzn83lZlj/84Q8/8YlPOI6DH6vr+te//vVl//b0Ps5wQD/77LNIl9M0RZAhTCVJ0jStUqmUZSnLcpZlvu8HQRAEAZLpIAgajUatVhNCIGSLokiSxDAMIcRgMEDmHcexYRhYvJvNZlmWkiSZphnH8XQ6xU979dVXl/0Y6D3OcMoxnU5VVS3LMo5jWZax3M7nc13X6/W6pmmqqiqKIssywrosyyzLDMNwXVeWZd/3sWDHcWxZFr4sjmNJkjqdjmEYx8fHcRzjB6qq2u/3wzDsdDqVSkXX9Wq1Gsfxpz/96SiKFEXRdd113evXry/7qdzr5GW/gH/fSy+95HletVrtdDr1et0wjCzLJpNJGIZlWQohJElKkiRJEiFEkiSLxSKOYxRAZFlWVXU+n0+nUyy9i8Vif39/Z2fn6OioLMtWq9VoNLD2NxoN27Yty1pbW6vX6wj9oijCMAzDMI7jLMuiKBqNRst+JHSWV2ghxJNPPol/fPGLX9R1HXGpaZplWbqum6aZpulwOPR9X5IkZBRFUURRVJaloihlWZqmqWlaFEVHR0fT6dS27Wq16vt+v9+fTqdxHK+srKytrSGsDcPAMi+ESNPUMIxms4mPBbxVlvksSAhxCnPo7e3tLMuyLPvyl7/8T37LZz7zGcMwLMsSQmia1mq1arVaWZaGYei6fnx8PJ/PLctKkmQ+n+NbENCapiEbCcPw+Ph4NpttbGy4rrtYLIQQvu+XZYldphAC6Y0sy9g+ZlmGIklRFJPJxPf90Wg0m81+8pOf/P88GPqnnK4V+vnnny+KwjCMJEmefvrpoiief/75f/wtjz/+uCzLmqZpmlav113XzfPc931FUeI4RmIgyzJy5SiKgiCwbXttbc2yrMVikaYp8hPTNA3DWF9f13V9NBrJsixJkqqquq73+33bthuNRhAEiqIURRHHMTo4aNNUKhVVVS3LajQa/5XnRP+rU7RCX79+PU1TSZIqlYqmaUVRZFmGSnCSJGmaPvfcc+/++qeeeqooiqIoEHaGYVSr1TRNB4OB4zibm5uapg2Hw/F4jCUWyW4QBNVqdWtrS9O08XgshJjNZkdHR6ZpXrhwod1uHx0d4T+xrquq+uabb1qWtb6+HkWRbduKovi+XxQF9oKqqmI3iZq37/vf/OY3l/IASZyeFfrZZ58VQjiOI0mSZVmu65ZlmaapEKIsSyyl169fH41G0+lUCIFV3DRNRNXKyoqu65PJBIurZVnD4VBRFMMwFEWZz+co8Om6/sADDzSbTcQr9nxFUeALfN+fTCZBEEiShIxCUZTZbHZ4eFir1Vqtlq7rZVkmSYLco1ar4WUoiqIoSpZlyFVoiU5LQM/n81qthv6Ibdu2bZdlmee5JEmo/qZpig96XddRKUOdDosoRjVms1kYhp7ntdttLOqu685ms+PjY9M0Pc/TNM00TfTGFUVBo8RxHCy9mqZh6XVd17bt1dXVZrO5t7f34IMP1ut1VVVR1CuKQgiByrdpmrqu45MESUie55/85Cd/8IMfLPuJ3qNOS0A3m01d1+fzeZZlaErruq4oCmLatm18rHuet76+3u/3EcpYvMVfcxLkyoPBAEU3dAezLEMpo1KpSJKEpDnLMqzQmqbt7u7+8Y9/bLfbq6ureMPg/2Ni6c9//nOe591udzqdpmlaq9VQtsMijW45VmghBPaLbJgv0WkJ6CRJkI/i4x61YSEEFuAkSZCqoo3yl7/85eDg4JFHHrly5cpsNuv3+8gZFEWRJAn7NgT9cDgMw9A0zTAM9/b2XNc1DCOOY1VVkU8j/s6dO7e6uopPBiHEZDKpVqvdbnc2mxmGcefOnb29PU3TThIhBH2apidvuaIo0D8Pw/DHP/7xsh/nveu0BLSqqpqmLRYLWZbn83kURZVKxbIspMjIgE/2Ya1Wa29vb2dnp9VqRVE0mUzQKMFKjDr0bDZDN1uWZdM0q9XqYDBA5S4IAsuyPM9zXbfZbD744INCCCQn6Gk3Go2NjQ1Zli9cuPDxj3/8Zz/72XA4bLVaKNKh+oFJD03TyrJESzLP8yRJWI1ertMS0FevXn3llVeQg2JwGRUPpAqmaSJQ0Bz54Ac/uLm56fv+cDicTqdIsouiwOxov99XVRUr6OrqqqIok8nEsizTNPf39yVJwvbOMAzbtlHpQ6ulVquhtLK+vl6tVoMg2NraunDhgm3beL8Nh0N8YmRZ9u4sH/3CNE37/f53v/vdZT/Le9ppCWghRL/fx4g9UlKcPUmSBMW40WhkmqZt28hJsB4PBoMois6dO+d5HurN6JXgbbBYLFzXRb6Bd8hoNErTtNPpnD9/fmVlBVW/MAwdxzmZWyqKIggC7P/w7lIU5fLly1mW3b59G68NbUIcFzAM42Qob9mPkE5TQF+7du0LX/iCZVmGYWDli6Lo+PgYQxpRFFWrVSEEtnSz2awsS9u2sQ+TZRld6PF4jH9rmobChaZp1Wp1b28vSRI0sTF1dN99933oQx/CRIcQwvf9Xq8XBAEqLXme93q9n/70p7/5zW9u3ry5urp68eLFPM/xQYECC7L8IAiw0QyC4Fvf+tZynyGdooAWQqDThsLcE0888aUvfWk2m/V6PXzQo6g8Ho97vV4YhhsbG/V6fXd3dzabOY7TaDQwPSf+ej6lWq02m03UAReLhe/7pmk2m01JkiaTCUIf75Y4jg8ODnq9Ht4nnuc1Go1Lly5JktRoNCzLevvtt5HVLBYL0zQxMI3EHZ8qo9Go1+st+/nRaeoUvttTTz2FpjdG+DEYZNs20lz0VrAkHx4eqqra7XZxUCWKItTXsHfc3NxENB8cHAyHQ+QP6PZtbW1tbm4i8z4+Pn799deLohgOh0EQdLvdSqWiKEoQBLdu3YqiCNtT0zTRcNE0DeVFFD3m8/nLL7+87GdGQpzagH63q1ev1mo1zBtVKhXUpFEgOxmdazQak8lkOByiOjGZTHCC8NKlS1tbW+PxeDgcapo2GAym02mj0ajX66urqyhLr6+v7+zsvPbaa3hL5HmOTMYwjFu3br3++uv3339/u90+ODio1Wpra2tI4k9OwSRJwl736XG6Uo739cILL2xvb+M4IGrMqFifDHKgWy6E8Dzv4sWLvV7v8PDQdd0wDN95553NzU0M7KPSZ1nWBz7wgUajge4jZuWSJNE0bTKZoDZy69atMAw/9rGPPfLII57neZ6nqmoQBLPZLMsyz/OQeyCl/s53vrPsJ0R/czYG/DHUhq5HWZboYqBpV5Zlu93WdV0I4TjO4eHhL37xi52dnWazWa/Xp9MpFnK0prvd7qVLlyqVihACh1mEEKhwow0exzFWcXQc8zy/fPkyvmVlZaXb7SqKMhwOj4+Px+NxnueM5tPmDKzQQgjf93FmBMmxEMI0zXa7jWH8xWJxdHSkKEoURXt7e/P5vNVqBUEwGAxQxev1eru7u/V6fW1tDd1yy7KQV2DJN01zbW0tiqKdnR1Jkh566CFFUXCA3DCMWq2WpulkMtE0Lc9zBHqe56+88sqyHwz9vbMR0F/96lexR8Q6igOw+/v7mJX77W9/+/Of/1ySJMdxarXaRz7ykfX1dQSi67qWZWH4syzL8Xicpmmr1VIUBe30arWKI+JFUbz99ttRFD300EOSJB0dHTmOs7q6itMotVrN8zzMQ2PaznGcZT8Veh9nYFN44oUXXnAcB/vCOI7feOMNXEiAmU90NzY3Ny9evIi5C2TYV65c2djYGAwG/X4fBeM8zzc2NjqdzmQyieMYZ7HefPPNX/7yl2traw8//PD+/v5kMrl06ZLrukmSeJ6X5/lrr722v7+PsbvBYIDmzvb29rKfCr3HWQpoIcSNGzc8z1MUxfM8wzAwsIEkZDAYoDWI9p6qqqqq1uv1ra0tVVV3dnYmkwkGQX/3u9/Ztv3Rj34UU3Xtdvvw8PBPf/rTYDA4d+6c67qHh4dlWaJtHkURJklu3rx58+ZN3L3U6/V832+32ysrK1/5yleW/VTob85GynHi0Ucf/d73vidJ0mw2M03z5HavIAgWiwXqHpVKpd1uY4rItu3BYICJvCRJRqMRYjQIgn6/32q1LMvK83yxWKA1g1ZLt9s9ODjApQV5nu/u7h4fHwdB4HleFEV/+MMf3njjDVmW19fXP/zhDy/7kdB7nLGAFkL0er1z587puo6YNgwjTdNKpXL//fcHQXDnzp3BYFCtVm3b1nUdtxRg+lnX9YODg9u3b2N7d+vWrUajMZvN0B/RNA1rOZZt/CPPc8/zut1uv98fj8c4JeC67uXLl9EjzPN82c+D3uOMpRwnXn311clkYts2ah2Yll4sFsgWEJG6ruMCA0mSptMpRvZ2d3dxHYfjOFeuXMEgKA6KX7x4ERtHVVX39/fjOG63261WS5KkX//612+99RYGkiqVCm5dGo/Hk8kkyzJe2nt6nL0VGj71qU997WtfG41Gi8Xivvvus20bF4diYR6Px3fu3MGMqOM4juPYto1i9srKCgaMMPXhOE4cx5VKpdPpzGaz27dvW5b18MMPoxodBAEO0YzHY9d12+02BpgwEo2KOEaU6JQ4qwEthHj88cdffvllxG6j0TipxKGNhz627/u4HQZc182y7J133nFdt1qt6rqOA4Loh+MsDJremPooyxIj1+Px2LKskxwDAyFoxPDCpFPlDAe0EOKxxx4TQty4cQOnAXCsEPch1ev10Wg0Go0wZoQrwnBOESdTUCdBvzAIgtFotLq6urW19fvf//5Xv/oVBpgwfo07PdI0xbluz/MqlcrJQQSm0afK2Wh9/2OPPvpov9/HJAbKHbi7EZc4YoBJCGGapuu63W73/PnzmOjHkBOuUJpOp77vdzqddruNg7e4tABz1fV63XEcy7I6nY7ruvjhmO7gmatT5e7J/65du4YDWrgdFHvE0WiEC41c1xVCYJLprbfe8n0fVzDiQK6u60mSOI6DInS/38eRWyEEbrJTFAX7TtyAg8PeuKX329/+9nJ/cXq3uyeg4fOf/7xhGJqmKYqCxACpMwb5kWHjUg5clYtT4ifnqXBxYxzHYRimaYqvPymboMuIAwE3btxY9u9K7+NuC+gTn/3sZ6vVquu6CG6cGD+Z05dl+ejoCFfg5XmOFAJfiS56GIa8zPwsumsDWgjxxBNP4AJSz/PQvhZCaJqGPxeEkhyai0ghNE1DYYRjdGfX3RzQ8Nhjj9VqNVybhBth0HM5+XMW6PkhmZ5Opy+99NKyXzL9++7+gIbPfe5z+LsTQggMM+GWAvyRipMBjxdffHHZr5T+T+6VgIarV69iQhpzpCfXjpVlib90SGfdvRXQQojt7W2kHzgRGMfx3107TWfaPRfQQohnnnkGF0A+88wzy34tRERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERERHQG/Q+l06GWjjQQTAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(image_embeddings.values())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmjV5qQ3suki",
        "outputId": "f439791f-e9af-4bb7-c9e1-eac7e28197de"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.35618728, 0.34230423, 0.09550171, ..., 0.03066884, 0.72184193,\n",
              "       0.16519433], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "with open('/mnt/gs/picture_embeddings/imagenet_resnets/raw_cutouts_imagenet_152_embeddings.pickle', 'wb') as handle:\n",
        "  pickle.dump(image_embeddings, handle, protocol = pickle.HIGHEST_PROTOCOL )"
      ],
      "metadata": {
        "id": "-LPw49hWAUeu"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}