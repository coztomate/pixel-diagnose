{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w5Moa8aYzmeg"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "182c68f7b4944246b71c4e7bd0d8db41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aad7e0d17ee43b78aca3ea95b221f96",
              "IPY_MODEL_c65c40b8f21d41c7b04bb2fc199d4f8f",
              "IPY_MODEL_6ba811f1fa934c2c882477c3b030ce5d"
            ],
            "layout": "IPY_MODEL_a831ce50377f411ea0d4cbc082dadefe"
          }
        },
        "8aad7e0d17ee43b78aca3ea95b221f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38aeb188797a4e36a2a234251b85da04",
            "placeholder": "​",
            "style": "IPY_MODEL_31d24d57f30e48b0883c146e237640f5",
            "value": "config.json: 100%"
          }
        },
        "c65c40b8f21d41c7b04bb2fc199d4f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d9f9e49855640b6b2eb189a4ec5c245",
            "max": 71813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b7084c541c64d4fa121bad4b3040640",
            "value": 71813
          }
        },
        "6ba811f1fa934c2c882477c3b030ce5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f5b2cda4c0647d291c7e96e74698e93",
            "placeholder": "​",
            "style": "IPY_MODEL_aba36a1f92f44707b07b9a9e9577d0bb",
            "value": " 71.8k/71.8k [00:00&lt;00:00, 2.65MB/s]"
          }
        },
        "a831ce50377f411ea0d4cbc082dadefe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38aeb188797a4e36a2a234251b85da04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d24d57f30e48b0883c146e237640f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d9f9e49855640b6b2eb189a4ec5c245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b7084c541c64d4fa121bad4b3040640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f5b2cda4c0647d291c7e96e74698e93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba36a1f92f44707b07b9a9e9577d0bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce37893da79479091b707b6f0577b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_830e8b2ed2a44b4e983d888cec7f2047",
              "IPY_MODEL_ea00fb7a99f24d14a854b33270aa1fdf",
              "IPY_MODEL_86a3d7e9f08e4c21b6fe3687d8a7ebab"
            ],
            "layout": "IPY_MODEL_1e7ae80f7ca944cb8c41fc14adc301aa"
          }
        },
        "830e8b2ed2a44b4e983d888cec7f2047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ed03361ef804cd0a395b8cdb5cf13cc",
            "placeholder": "​",
            "style": "IPY_MODEL_68a42fbb279e46c5b523ffb4221f2211",
            "value": "model.safetensors: 100%"
          }
        },
        "ea00fb7a99f24d14a854b33270aa1fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea2be2fd3ece434f9f57b897c92ffdbc",
            "max": 113412768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57a3ec0bbfe34e23a7802acbcfce8b75",
            "value": 113412768
          }
        },
        "86a3d7e9f08e4c21b6fe3687d8a7ebab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eef1ef423d34726b3d8f67cd1a1ecdc",
            "placeholder": "​",
            "style": "IPY_MODEL_afde0e187cf340a58065510e3af23e02",
            "value": " 113M/113M [00:01&lt;00:00, 132MB/s]"
          }
        },
        "1e7ae80f7ca944cb8c41fc14adc301aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ed03361ef804cd0a395b8cdb5cf13cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a42fbb279e46c5b523ffb4221f2211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea2be2fd3ece434f9f57b897c92ffdbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a3ec0bbfe34e23a7802acbcfce8b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9eef1ef423d34726b3d8f67cd1a1ecdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afde0e187cf340a58065510e3af23e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "w5Moa8aYzmeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies.\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt -qq update && apt -qq install gcsfuse\n",
        "\n",
        "!pip install pydicom matplotlib transformers nibabel\n",
        "\n",
        "# Authenticate.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YECkGx2bHVAo",
        "outputId": "c4f21a4e-c074-4e9c-c233-81ef8e98dda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "100  2659  100  2659    0     0  21151      0 --:--:-- --:--:-- --:--:-- 21272\n",
            "OK\n",
            "19 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 5,561 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 120882 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_1.2.1_amd64.deb ...\n",
            "Unpacking gcsfuse (1.2.1) ...\n",
            "Setting up gcsfuse (1.2.1) ...\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.4.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel) (67.7.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: pydicom\n",
            "Successfully installed pydicom-2.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##!mkdir \"/content/brats\"\n",
        "#!mkdir \"/content/upenn\"\n",
        "\n",
        "#!gcsfuse \"brats-image-files-eu\" \"/content/brats\"\n",
        "#!gcsfuse \"upenn-gbm-nifti\" \"/content/upenn\"\n",
        "\n",
        "#!fusermount -u /content/brats"
      ],
      "metadata": {
        "id": "58cSuO9KoBhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions"
      ],
      "metadata": {
        "id": "-eFXl9xZL8zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "9ybg_0tzz5qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def find_scan_and_segm_files_gcs(bucket_path,dataset):\n",
        "    '''\n",
        "    Finds the scan and segmentation files for a given scan type in a GCS bucket\n",
        "\n",
        "    Args:\n",
        "        bucket_path (string): Path to the GCS bucket (e.g., 'gs://my-bucket/')\n",
        "        scan_type (string): Type of scan. Can be 'flair', 't1', 't1gd', or 't2'\n",
        "    Returns:\n",
        "        scan_path (string): Path to the scan file in GCS\n",
        "        seg_path (string): Path to the segmentation file in GCS\n",
        "    '''\n",
        "\n",
        "    patient_id = \"\".join((bucket_path).split(\"/\")[-2:-1])\n",
        "\n",
        "    # Define scan path keys\n",
        "    scan_paths = {\n",
        "        f't1': None,   # Corresponds to 't1' in UPenn and 't1n' in Brats\n",
        "        f't1c': None,  # Corresponds to 't1gd' in UPenn and 't1c' in Brats\n",
        "        f't2': None,   # Corresponds to 't2' in UPenn and 't2w' in Brats\n",
        "        f'flair': None,  # Corresponds to 'flair' in UPenn and 't2f' in Brats\n",
        "        f'seg': None   # Segmentation file\n",
        "    }\n",
        "\n",
        "    # Use gsutil to list files in the bucket\n",
        "    cmd = f'gsutil ls -r \"{bucket_path}**\"'\n",
        "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    out, err = process.communicate()\n",
        "\n",
        "    if process.returncode != 0:\n",
        "        print(\"Error:\", err.decode('utf-8'))\n",
        "        return None\n",
        "\n",
        "    # Decode output and split into lines\n",
        "    files = out.decode('utf-8').splitlines()\n",
        "\n",
        "    # Search for the files\n",
        "    for file in files:\n",
        "        if dataset == 'brats':\n",
        "            # File matching for 'brats'\n",
        "            if 't1n.nii.gz' in file:\n",
        "                scan_paths[f't1'] = file\n",
        "            elif 't1c.nii.gz' in file:\n",
        "                scan_paths[f't1c'] = file\n",
        "            elif 't2w.nii.gz' in file:\n",
        "                scan_paths[f't2'] = file\n",
        "            elif 't2f.nii.gz' in file:\n",
        "                scan_paths[f'flair'] = file\n",
        "            elif 'seg.nii.gz' in file:\n",
        "                scan_paths[f'seg'] = file\n",
        "        elif dataset == 'upenn':\n",
        "            # File matching for 'upenn'\n",
        "            if '_11_segm.nii' in file:\n",
        "                scan_paths[f'seg'] = file\n",
        "            else:\n",
        "                for upenn_type, brats_type in [('flair', 'flair'), ('t1', 't1'), ('t1gd', 't1c'), ('t2', 't2')]:\n",
        "                    scan_file_pattern = f'_11_{upenn_type.upper()}.nii.gz'\n",
        "                    if file.endswith(scan_file_pattern):\n",
        "                        scan_paths[f'{brats_type}'] = file\n",
        "\n",
        "    return patient_id, scan_paths\n",
        "\n"
      ],
      "metadata": {
        "id": "BXm0qa7DgW5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_nii(file_name):\n",
        "    '''\n",
        "    Reads a NIfTI file and returns the data as a numpy array\n",
        "    '''\n",
        "\n",
        "    # reads the NIfTI file\n",
        "    nii_file = nib.load(file_name)\n",
        "\n",
        "    # Access the data\n",
        "    data = nii_file.get_fdata()\n",
        "    return data\n",
        "\n",
        "# Load the original brain scan and the segmentation mask\n",
        "def mask_slice(original_data, mask_data):\n",
        "        '''\n",
        "        Function to display the slice with the largest affected\n",
        "        area of the original MRI scan together with slice of segmentation mask\n",
        "\n",
        "        Args:\n",
        "            original_data (numpy array): original MRI scan (already loaded from NIfTI file)\n",
        "            mask_data (numpy array): segmentation mask (already loaded from NIfTI file)\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "\n",
        "        # Initialize variables to track the largest slice\n",
        "        max_non_black_count = 0\n",
        "        max_slice_index = 0\n",
        "\n",
        "        # Iterate through each slice in the mask\n",
        "        for i in range(mask_data.shape[2]):\n",
        "            # Count non-black (non-zero) pixels in the slice\n",
        "            non_black_count = np.count_nonzero(mask_data[:, :, i])\n",
        "\n",
        "            # Update max count and slice index if current slice has more non-black pixels\n",
        "            if non_black_count > max_non_black_count:\n",
        "                max_non_black_count = non_black_count\n",
        "                max_slice_index = i\n",
        "\n",
        "        ## Get bounding-box from mask\n",
        "        mask_array = mask_data[:,:,max_slice_index]\n",
        "        # Identifying the indices of non-zero elements\n",
        "        non_zero_indices = np.argwhere(mask_array != 0)\n",
        "\n",
        "        # Finding the min and max indices along x and y axis\n",
        "        min_y, min_x = non_zero_indices.min(axis=0)\n",
        "        max_y, max_x = non_zero_indices.max(axis=0)\n",
        "\n",
        "        # Creating the bounding box mask\n",
        "        mask = np.zeros_like(mask_array)\n",
        "        mask[min_y:max_y + 1, min_x:max_x + 1] = 1\n",
        "\n",
        "        return max_slice_index,mask"
      ],
      "metadata": {
        "id": "Nqhp6hWMiinf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "\n",
        "# Adjust image range between 0 and max value for 16 bits\n",
        "def preprocess_image(best_slice):\n",
        "\n",
        "  # Normalize to 0-255\n",
        "  min_val = best_slice.min()\n",
        "  max_val = best_slice.max()\n",
        "  image_array = (best_slice - min_val) / (max_val - min_val) * 255\n",
        "  image_array = np.uint8(image_array)\n",
        "\n",
        "  # Make Greyscale to RGB\n",
        "  image_array = np.stack((image_array,) * 3, axis=-1)\n",
        "  image = Image.fromarray(image_array)\n",
        "\n",
        "  # Turn into tensor and add one dimension for \"batch-size\"\n",
        "  transform = transforms.ToTensor()\n",
        "  image_tensor = transform(image).unsqueeze(0) # Shape: 1,x,x,x\n",
        "\n",
        "  return image, image_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "0QU-GgRlkcps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "Q-cFMWqCz29w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import io\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "\n",
        "def load_models():\n",
        "\n",
        "  ### Load weights\n",
        "\n",
        "  url = \"https://storage.googleapis.com/pytrial/medclip-pretrained.zip\"\n",
        "  r = requests.get(url)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall(\"RESNET50_wb\")\n",
        "\n",
        "  # See in https://github.com/mk-statistics/MedCLIP/blob/main/medclip/modeling_medclip.py line 66-70\n",
        "  state_dict_resnet = torch.load(\"RESNET50_wb/pytorch_model.bin\",map_location=torch.device('cpu'))\n",
        "  new_state_dict_resnet = {}\n",
        "  for key in state_dict_resnet.keys():\n",
        "    if 'vision_model' in key:\n",
        "      new_state_dict_resnet[key.replace('vision_model.model.','')] = state_dict_resnet[key]\n",
        "\n",
        "  url = \"https://storage.googleapis.com/pytrial/medclip-vit-pretrained.zip\"\n",
        "  r = requests.get(url)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall(\"ViT_wb\")\n",
        "\n",
        "  # Do transforms on weights\n",
        "  # See in https://github.com/mk-statistics/MedCLIP/blob/main/medclip/modeling_medclip.py line 109-113\n",
        "  state_dict_vit = torch.load(\"ViT_wb/pytorch_model.bin\",map_location=torch.device('cpu'))\n",
        "  new_state_dict_vit = {}\n",
        "  for key in state_dict_vit.keys():\n",
        "    if 'vision_model' in key:\n",
        "      new_state_dict_vit[key.replace('vision_model.model.','')] = state_dict_vit[key]\n",
        "\n",
        "  new_state_dict_vit[\"projection_head.weight\"] = new_state_dict_vit.pop(\"vision_model.projection_head.weight\")\n",
        "\n",
        "\n",
        "  ### Initilize models\n",
        "\n",
        "  model_resnet = models.resnet50(weights=None)\n",
        "\n",
        "  # Add one layer as the projection head as seen in https://github.com/mk-statistics/MedCLIP/blob/main/medclip/modeling_medclip.py line 51,52,53\n",
        "  projection_head = nn.Linear(model_resnet.fc.in_features, 512, bias=False) # create an embedding\n",
        "  model_resnet.fc = projection_head\n",
        "\n",
        "  model_resnet.load_state_dict(new_state_dict_resnet)\n",
        "\n",
        "\n",
        "\n",
        "  model_vit = AutoModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "  projection_head = nn.Linear(768, 512, bias=False) # Add projection_head in https://github.com/mk-statistics/MedCLIP/blob/main/medclip/modeling_medclip.py line 95\n",
        "  model_vit.projection_head = projection_head\n",
        "\n",
        "  model_vit.load_state_dict(new_state_dict_vit)\n",
        "\n",
        "  return model_resnet, model_vit\n",
        "\n",
        "\n",
        "def do_inference(model_name,model,image_tensor):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    if model_name == \"resnet50\":\n",
        "\n",
        "      vis_output = model(image_tensor)\n",
        "      embedding = vis_output / vis_output.norm(dim=-1, keepdim=True) # See in https://github.com/mk-statistics/MedCLIP/blob/main/medclip/modeling_medclip.py line 199\n",
        "\n",
        "    elif model_name == \"vit\":\n",
        "\n",
        "      vis_output = model.projection_head(model(image_tensor)[\"pooler_output\"]) # Add forward pass to the added projection_layer... actually could not add it to the forward() method\n",
        "      embedding = vis_output / vis_output.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "  return embedding.numpy().reshape((512,))\n"
      ],
      "metadata": {
        "id": "jGTDFo6gImnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/christiansafka/img2vec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "class Img2Vec():\n",
        "    RESNET_OUTPUT_SIZES = {\n",
        "        'resnet18': 512,\n",
        "        'resnet34': 512,\n",
        "        'resnet50': 2048,\n",
        "        'resnet101': 2048,\n",
        "        'resnet152': 2048,\n",
        "    }\n",
        "\n",
        "    def __init__(self, cuda=False, model='resnet-34', layer='default',\n",
        "                 layer_output_size=512):\n",
        "        \"\"\" Img2Vec\n",
        "        :param cuda: If set to True, will run forward pass on GPU\n",
        "        :param model: String name of requested model\n",
        "        :param layer: String or Int depending on model.  See more docs: https://github.com/christiansafka/img2vec.git\n",
        "        :param layer_output_size: Int depicting the output size of the requested layer\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "        self.layer_output_size = layer_output_size\n",
        "        self.model_name = model\n",
        "\n",
        "        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        self.scaler = transforms.Resize((224, 224))\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def get_vec(self, img, tensor=False):\n",
        "        \"\"\" Get vector embedding from PIL image\n",
        "        :param img: PIL Image or list of PIL Images\n",
        "        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n",
        "        :returns: Numpy ndarray\n",
        "        \"\"\"\n",
        "        if type(img) == list:\n",
        "            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n",
        "            images = torch.stack(a).to(self.device)\n",
        "            if self.model_name == 'alexnet':\n",
        "                my_embedding = torch.zeros(len(img), self.layer_output_size)\n",
        "            else:\n",
        "                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n",
        "\n",
        "            def copy_data(m, i, o):\n",
        "                my_embedding.copy_(o.data)\n",
        "\n",
        "            h = self.extraction_layer.register_forward_hook(copy_data)\n",
        "            h_x = self.model(images)\n",
        "            h.remove()\n",
        "\n",
        "            if tensor:\n",
        "                return my_embedding\n",
        "            else:\n",
        "                if self.model_name == 'alexnet':\n",
        "                    return my_embedding.numpy()[:, :]\n",
        "                else:\n",
        "                    return my_embedding.numpy()[:, :, 0, 0]\n",
        "        else:\n",
        "            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(self.device)\n",
        "\n",
        "            if self.model_name == 'alexnet':\n",
        "                my_embedding = torch.zeros(1, self.layer_output_size)\n",
        "            else:\n",
        "                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n",
        "\n",
        "            def copy_data(m, i, o):\n",
        "                my_embedding.copy_(o.data)\n",
        "\n",
        "            h = self.extraction_layer.register_forward_hook(copy_data)\n",
        "            h_x = self.model(image)\n",
        "            h.remove()\n",
        "\n",
        "            if tensor:\n",
        "                return my_embedding\n",
        "            else:\n",
        "                if self.model_name == 'alexnet':\n",
        "                    return my_embedding.numpy()[0, :]\n",
        "                else:\n",
        "                    return my_embedding.numpy()[0, :, 0, 0]\n",
        "\n",
        "    def _get_model_and_layer(self, model_name, layer):\n",
        "        \"\"\" Internal method for getting layer from model\n",
        "        :param model_name: model name such as 'resnet-18'\n",
        "        :param layer: layer as a string for resnet-18 or int for alexnet\n",
        "        :returns: pytorch model, selected layer\n",
        "        \"\"\"\n",
        "\n",
        "        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n",
        "            model = getattr(models, model_name)(pretrained=True)\n",
        "            if layer == 'default':\n",
        "                layer = model._modules.get('avgpool')\n",
        "                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n",
        "            else:\n",
        "                layer = model._modules.get(layer)\n",
        "            return model, layer\n",
        "        elif model_name == 'resnet-34':\n",
        "            model = models.resnet34(pretrained=True)\n",
        "            if layer == 'default':\n",
        "                layer = model._modules.get('avgpool')\n",
        "                self.layer_output_size = 512\n",
        "            else:\n",
        "                layer = model._modules.get(layer)\n",
        "\n",
        "            return model, layer\n",
        "\n",
        "        elif model_name == 'alexnet':\n",
        "            model = models.alexnet(pretrained=True)\n",
        "            if layer == 'default':\n",
        "                layer = model.classifier[-2]\n",
        "                self.layer_output_size = 4096\n",
        "            else:\n",
        "                layer = model.classifier[-layer]\n",
        "\n",
        "            return model, layer\n",
        "\n",
        "        else:\n",
        "            raise KeyError('Model %s was not found' % model_name)"
      ],
      "metadata": {
        "id": "LN-6FSKSc6xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main-Function"
      ],
      "metadata": {
        "id": "JH3w5ibZz9hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Models"
      ],
      "metadata": {
        "id": "6YZ6QNe00AR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "model_resnet, model_vit = load_models()\n",
        "img2vec50 = Img2Vec(model='resnet50')\n",
        "img2vec152 = Img2Vec(model='resnet152')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285,
          "referenced_widgets": [
            "182c68f7b4944246b71c4e7bd0d8db41",
            "8aad7e0d17ee43b78aca3ea95b221f96",
            "c65c40b8f21d41c7b04bb2fc199d4f8f",
            "6ba811f1fa934c2c882477c3b030ce5d",
            "a831ce50377f411ea0d4cbc082dadefe",
            "38aeb188797a4e36a2a234251b85da04",
            "31d24d57f30e48b0883c146e237640f5",
            "0d9f9e49855640b6b2eb189a4ec5c245",
            "0b7084c541c64d4fa121bad4b3040640",
            "9f5b2cda4c0647d291c7e96e74698e93",
            "aba36a1f92f44707b07b9a9e9577d0bb",
            "1ce37893da79479091b707b6f0577b6e",
            "830e8b2ed2a44b4e983d888cec7f2047",
            "ea00fb7a99f24d14a854b33270aa1fdf",
            "86a3d7e9f08e4c21b6fe3687d8a7ebab",
            "1e7ae80f7ca944cb8c41fc14adc301aa",
            "3ed03361ef804cd0a395b8cdb5cf13cc",
            "68a42fbb279e46c5b523ffb4221f2211",
            "ea2be2fd3ece434f9f57b897c92ffdbc",
            "57a3ec0bbfe34e23a7802acbcfce8b75",
            "9eef1ef423d34726b3d8f67cd1a1ecdc",
            "afde0e187cf340a58065510e3af23e02"
          ]
        },
        "id": "1IiUbQtWjuBW",
        "outputId": "9a04aee6-ab9c-4281-cab1-b0a3c4039dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "182c68f7b4944246b71c4e7bd0d8db41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ce37893da79479091b707b6f0577b6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:02<00:00, 37.3MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:02<00:00, 100MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data to iterate trough"
      ],
      "metadata": {
        "id": "l-S54Kyi0E9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "#!gsutil ls -d gs://brats-image-files-eu/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/*-000/ #BRATS_GLI # /ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData/ #ASNR-MICCAI-BraTS2023-PED-Challenge-TrainingData/ BraTS-MEN-Train/\n",
        "#!gsutil ls -d gs://brats-image-files-eu/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData/*-000/ ##!gsutil ls -d gs://upenn-gbm-nifti/PKG-UPENN-GBM-NIfTI/UPENN-GBM/NIfTI-files/images_structural/*_11/ # UPENN GLIO\n",
        "\n",
        "#cmd = f'gsutil ls gs://upenn-gbm-nifti/PKG-UPENN-GBM-NIfTI/UPENN-GBM/NIfTI-files/images_structural/'\n",
        "cmd = f'gsutil ls gs://brats-image-files-eu/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData/'#*-000/'\n",
        "process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "out = process.communicate()[0].decode('utf-8').splitlines()\n",
        "dataset = \"brats\"\n",
        "diagnose = \"met\"\n"
      ],
      "metadata": {
        "id": "SM8IxwwWiSvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do the actucal inference, store reference images onto GCS & store embeddings into dict"
      ],
      "metadata": {
        "id": "jhFAsOBK0Icv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import hashlib\n",
        "import uuid\n",
        "import torch\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "embedding_dict = {}\n",
        "\n",
        "start = time.time()\n",
        "### MAIN\n",
        "for counter,pat_path in enumerate(out):\n",
        "\n",
        "  # Get files\n",
        "  if pat_path == \"gs://brats-image-files-eu/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData/\":\n",
        "    continue\n",
        "\n",
        "  uui = str(uuid.uuid4())\n",
        "  embedding_dict[uui] = {}\n",
        "\n",
        "  patid, files = find_scan_and_segm_files_gcs(pat_path,dataset)\n",
        "\n",
        "  # Store in dict\n",
        "  embedding_dict[uui][\"pat_id\"] = patid\n",
        "  embedding_dict[uui][\"bucket_url\"] = pat_path\n",
        "  embedding_dict[uui][\"origin_ds\"] = dataset\n",
        "  embedding_dict[uui][\"diagnose\"] = diagnose\n",
        "\n",
        "  # Download files loacally\n",
        "  !gcloud storage cp -r {pat_path} .\n",
        "\n",
        "  ## Now do the embedding for every image\n",
        "\n",
        "  seg = read_nii(\"/\".join(files[\"seg\"].split(\"/\")[-2:]))\n",
        "  slice_index,mask = mask_slice(read_nii(\"/\".join(files[\"t1\"].split(\"/\")[-2:])), seg)\n",
        "\n",
        "  # Store in dict\n",
        "  embedding_dict[uui][\"embeddings\"] = {}\n",
        "  embedding_dict[uui][\"embeddings\"][\"slice_id\"] = slice_index\n",
        "  embedding_dict[uui][\"embeddings\"][\"RESNET50_MEDCLIP\"] = {}\n",
        "  embedding_dict[uui][\"embeddings\"][\"VIT_MEDCLIP\"] = {}\n",
        "  embedding_dict[uui][\"embeddings\"][\"RESNET50_IMAGENET\"] = {}\n",
        "  embedding_dict[uui][\"embeddings\"][\"RESNET152_IMAGENET\"] = {}\n",
        "\n",
        "  for i in [\"t1\",\"t1c\",\"t2\",\"flair\"]:\n",
        "\n",
        "    pic = read_nii(\"/\".join(files[i].split(\"/\")[-2:]))[:,:,slice_index]\n",
        "    pic_seg = pic * mask\n",
        "\n",
        "    # Precprocess image\n",
        "    image, image_tensor = preprocess_image(pic)\n",
        "    seg_image, seg_image_tensor = preprocess_image(pic_seg)\n",
        "\n",
        "    # Save image an upload to google\n",
        "    image_path = patid + \"/png/\"\n",
        "    !mkdir -p {image_path}\n",
        "    image.save((image_path + patid + \"_\" + i + \"_sliced.png\"))\n",
        "    seg_image.save((image_path + patid + \"_seg_\" + i + \"_sliced.png\"))\n",
        "\n",
        "    # Create embedding w medclip\n",
        "    resnet_embedding = do_inference(\"resnet50\",model_resnet,image_tensor)\n",
        "    seg_resnet_embedding = do_inference(\"resnet50\",model_resnet,seg_image_tensor)\n",
        "    embedding_dict[uui][\"embeddings\"][\"RESNET50_MEDCLIP\"][i] = resnet_embedding\n",
        "    embedding_dict[uui][\"embeddings\"][\"RESNET50_MEDCLIP\"][\"seg_\" + i] = seg_resnet_embedding\n",
        "\n",
        "    vit_embedding = do_inference(\"vit\",model_vit,image_tensor)\n",
        "    seg_vit_embedding = do_inference(\"vit\",model_vit,seg_image_tensor)\n",
        "    embedding_dict[uui][\"embeddings\"][\"VIT_MEDCLIP\"][i] = vit_embedding\n",
        "    embedding_dict[uui][\"embeddings\"][\"VIT_MEDCLIP\"][\"seg_\" + i] = seg_vit_embedding\n",
        "\n",
        "    # Create embedding w resnet\n",
        "    resnet50_imagenet_emedding = img2vec50.get_vec(image)\n",
        "    seg_resnet50_imagenet_emedding = img2vec50.get_vec(seg_image)\n",
        "    embedding_dict[uui][\"embeddings\"][\"RESNET50_IMAGENET\"][i] = resnet50_imagenet_emedding\n",
        "    embedding_dict[uui][\"embeddings\"][\"RESNET50_IMAGENET\"][\"seg_\" + i] = seg_resnet50_imagenet_emedding\n",
        "\n",
        "    resnet152_imagenet_emedding = img2vec152.get_vec(image)\n",
        "    seg_resnet152_imagenet_emedding = img2vec152.get_vec(seg_image)\n",
        "    embedding_dict[uui][\"embeddings\"][\"RESNET152_IMAGENET\"][i] = resnet152_imagenet_emedding\n",
        "    embedding_dict[uui][\"embeddings\"][\"RESNET152_IMAGENET\"][\"seg_\" + i] = seg_resnet152_imagenet_emedding\n",
        "\n",
        "  !gsutil -m cp -R {image_path} {pat_path}\n",
        "\n",
        "\n",
        "  # After one cycle: remove data again\n",
        "  !rm -rf {patid}\n",
        "\n",
        "  if counter >= 100:\n",
        "\n",
        "    with open('100_ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData_embedding.pickle', 'wb') as handle:\n",
        "      pickle.dump(embedding_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    !gsutil -m cp  /content/100_ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData_embedding.pickle gs://picture_embeddings/combined_embeddings\n",
        "\n",
        "    break\n",
        "\n",
        "  print(\"Counter: \" + str(counter))\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "qrRgFuw1DGc5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}